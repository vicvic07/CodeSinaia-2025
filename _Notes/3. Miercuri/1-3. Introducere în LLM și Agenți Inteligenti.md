uMiercuri:   9:00 - 10:45 (A)
        10:45 - 12:30 (B) 
Dependențe: [[4. Hands-on cu Python și Jupyter Notebooks]]
Resources: [Ollama](https://ollama.com/download)

- [ ] Finalizăm instalarea serverului [Ollama](https://ollama.com/download) și a unui LLM care să se potrivească cu resursele sistemului. Instrucțiuni de instalare: [[7. Ollama]]
	Opțiuni pt modele:
		- **gemma3:1b** - 1 miliard de parameteri, ~815Mb. Should run on computers with >8Gb of RAM
		- **llama3.2** - 3 miliarde de parametri, ~2.5Gb. Should work on computers with > 16Gb of RAM
		- **mistral** - 4 miliarde de parametri, ~4Gb. Needs ~5Gb of *free* RAM to work (failed on a computers with 16Gb in total, running on 3.6Gb free)
	Instalarea se poate face complet offline, folosind unul din USB stick-urile inproted.
	Copierea modelului selectat se face de pe USB stick, în directorul C:/Users/YourUser/.ollama (sau ~/.ollama în linux)
- [ ] Discuție liberă despre câțiva termeni specifici ML și LLM:
	- Ce înseamnă parametri ai unui model, tensor, token și tokenizer, embedding, encoder, transformer & decoder.

- [ ] Code review pe programul de test: [HelloOllama.py](https://github.com/inproted/CodeSinaia-2025/blob/main/HelloOllama.py)
 
---
Codăm împreună: **smart-chat.py**: implementăm o un buclă de comenzi, în care user-ul să poată dialoga cu modelul instalat în Ollama.
- [ ] Prima iterație: folosim ollama.generate() care nu reține istoricul conversației.
	```
	prompt = input de la user
	cât timp prompt-ul nu este '/bye':
	    dacă prompt-ul nu este '':
	        răspuns = ollama.generate(prompt)
		    print răspuns
		prompt = input de la user
	```
- [ ] Iterația a doua: acumulăm prompt-urile de la user într-un full-prompt, pe care il pasăm lui **ollama.chat()**.
- [ ] Iterația a treia: acumulăm atât prompt-urile de la user cât și răspunsurile de la agent într-un vector de {'role':'agent/user', 'content':text} pe care le pasăm lui **ollama.chat**(). Dăm în acest fel un context agentului din istoricul conversației într-un mod structurat.
- [ ] Iterația a patra: extragem codul care interfațează cu ollama într-o clasă distinctă. Introducem conceptul de clasă în python.
- [ ] Iterația a cincea: inserăm în dialog un prim prompt de context citit din fișierul [context_prompt.txt](https://github.com/inproted/CodeSinaia-2025/blob/main/IntroToLLM/context_prompt.txt)
---
- [ ] Discuție și code review: SentenceTransformer și construirea contextului prin embeddings și similaritate (George)  